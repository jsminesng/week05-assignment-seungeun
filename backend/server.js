/**
 * Express AI Proxy Server for Week 5
 *
 * ì´ ì„œë²„ì˜ ì—­í• :
 * 1. í”„ë¡ íŠ¸ì—”ë“œì™€ LLM API ì‚¬ì´ì˜ í”„ë¡ì‹œ (API í‚¤ ë³´í˜¸)
 * 2. Rate Limiting (ë¹„ìš© ê´€ë¦¬)
 * 3. ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
 * 4. ì—¬ëŸ¬ LLM ì œê³µìž ì§€ì› (Gemini, OpenAI, Hugging Face)
 *
 * ì‹¤í–‰ ë°©ë²•:
 * 1. npm install
 * 2. .env íŒŒì¼ ì„¤ì •
 * 3. node server.js
 */

require("dotenv").config();
const express = require("express");
const cors = require("cors");
const rateLimit = require("express-rate-limit");

const app = express();
const PORT = process.env.PORT || 3002;

// ===================================
// ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
// ===================================

// CORS í™œì„±í™” (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)
app.use(cors());

// JSON íŒŒì‹±
app.use(express.json());

// ë¡œê¹… ë¯¸ë“¤ì›¨ì–´
app.use((req, res, next) => {
  console.log(`[${new Date().toISOString()}] ${req.method} ${req.path}`);
  next();
});

// Rate Limiting (ë¹„ìš© ê´€ë¦¬!)
const aiLimiter = rateLimit({
  windowMs: 1 * 60 * 1000, // 1ë¶„
  max: 10, // 1ë¶„ì— 10ë²ˆ í—ˆìš© (í…ŒìŠ¤íŠ¸/í•™ìŠµìš©ìœ¼ë¡œ ì™„í™”)
  message: {
    success: false,
    error: "Too many AI requests. Please try again later.",
  },
  standardHeaders: true,
  legacyHeaders: false,
});

// ===================================
// LLM ì„¤ì • (Multi-Provider Support)
// ===================================

const LLM_CONFIGS = {
  gemini: {
    name: "Google Gemini",
    async generate(prompt) {
      const { GoogleGenerativeAI } = require("@google/generative-ai");
      const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
      const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });

      const result = await model.generateContent(prompt);
      return result.response.text();
    },
  },
  openai: {
    name: "OpenAI",
    async generate(prompt) {
      const { OpenAI } = require("openai");
      const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

      const completion = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [{ role: "user", content: prompt }],
        max_tokens: 250,
        temperature: 0.7,
      });

      return completion.choices[0].message.content;
    },
  },
  huggingface: {
    name: "Hugging Face",
    async generate(prompt) {
      const response = await fetch(
        `https://api-inference.huggingface.co/models/${process.env.HUGGINGFACE_MODEL}`,
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${process.env.HUGGINGFACE_API_KEY}`,
            "Content-Type": "application/json",
          },
          body: JSON.stringify({
            inputs: prompt,
            parameters: {
              max_new_tokens: 250,
              temperature: 0.7,
              top_p: 0.95,
            },
          }),
        }
      );

      if (!response.ok) {
        const error = await response.json();
        throw new Error(error.error || "Hugging Face API error");
      }

      const data = await response.json();
      return data[0].generated_text;
    },
  },
};

// ===================================
// API ì—”ë“œí¬ì¸íŠ¸
// ===================================

// í—¬ìŠ¤ ì²´í¬
app.get("/", (req, res) => {
  res.json({
    status: "OK",
    message: "Week 5 AI Proxy Server",
    provider: process.env.LLM_PROVIDER || "gemini",
    timestamp: new Date().toISOString(),
  });
});

// AI ìƒì„± ì—”ë“œí¬ì¸íŠ¸ (Rate Limited)
app.post("/api/ai/generate", aiLimiter, async (req, res) => {
  const { prompt } = req.body;

  // ìž…ë ¥ ê²€ì¦
  if (!prompt || typeof prompt !== "string") {
    return res.status(400).json({
      success: false,
      error: "Prompt is required and must be a string",
    });
  }

  if (prompt.length > 2000) {
    return res.status(400).json({
      success: false,
      error: "Prompt is too long (max 2000 characters)",
    });
  }

  // LLM ì œê³µìž ì„ íƒ
  const provider = process.env.LLM_PROVIDER || "gemini";
  const config = LLM_CONFIGS[provider];

  if (!config) {
    return res.status(500).json({
      success: false,
      error: `Unsupported LLM provider: ${provider}`,
    });
  }

  // API í‚¤ í™•ì¸
  const apiKey = process.env[`${provider.toUpperCase()}_API_KEY`];
  if (!apiKey) {
    console.error(`Missing API key for provider: ${provider}`);
    return res.status(500).json({
      success: false,
      error: "LLM API key not configured",
    });
  }

  console.log(`[AI Request] Provider: ${config.name}`);
  console.log(`[AI Request] Prompt: ${prompt.slice(0, 100)}...`);

  try {
    const startTime = Date.now();

    // LLM API í˜¸ì¶œ
    const text = await config.generate(prompt);

    const duration = Date.now() - startTime;
    console.log(`[AI Response] Success (${duration}ms)`);

    res.json({
      success: true,
      text,
      provider: config.name,
      duration,
    });
  } catch (error) {
    console.error("[AI Error]", error);

    // ì—ëŸ¬ ë©”ì‹œì§€ ì •ì œ
    let errorMessage = error.message;

    if (error.message.includes("API key")) {
      errorMessage = "Invalid API key. Please check your configuration.";
    } else if (error.message.includes("quota")) {
      errorMessage = "API quota exceeded. Please try again later.";
    } else if (error.message.includes("rate limit")) {
      errorMessage = "Rate limit exceeded. Please try again later.";
    }

    res.status(500).json({
      success: false,
      error: errorMessage,
      provider: config.name,
    });
  }
});

// í• ì¼ ë¶„í•´ ì „ìš© ì—”ë“œí¬ì¸íŠ¸ (í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í¬í•¨)
app.post("/api/ai/breakdown", aiLimiter, async (req, res) => {
  const { task } = req.body;

  if (!task || typeof task !== "string") {
    return res.status(400).json({
      success: false,
      error: "Task is required and must be a string",
    });
  }

  // í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
  const prompt = `ë‹¤ìŒ í• ì¼ì„ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ 5ê°œ ì´í•˜ì˜ ìž‘ì€ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì„¸ìš”:
"${task}"

ê° ë‹¨ê³„ëŠ” í•œ ì¤„ë¡œ ìž‘ì„±í•˜ê³ , ë²ˆí˜¸ë¥¼ ë¶™ì´ì§€ ë§ˆì„¸ìš”.
ì‹¤í–‰ ê°€ëŠ¥í•œ ë™ì‚¬ë¡œ ì‹œìž‘í•˜ì„¸ìš” (ì˜ˆ: "~í•˜ê¸°", "~ìž‘ì„±í•˜ê¸°").
ê°„ê²°í•˜ê²Œ ìž‘ì„±í•˜ì„¸ìš” (ê° ë‹¨ê³„ëŠ” í•œ ë¬¸ìž¥).`;

  // generate ì—”ë“œí¬ì¸íŠ¸ ìž¬ì‚¬ìš©
  req.body = { prompt };
  return app._router.handle(req, res, () => {});
});

// 404 ì²˜ë¦¬
app.use((req, res) => {
  res.status(404).json({
    success: false,
    error: "Endpoint not found",
  });
});

// ì—ëŸ¬ í•¸ë“¤ëŸ¬
app.use((err, req, res, next) => {
  console.error("[Server Error]", err);
  res.status(500).json({
    success: false,
    error: "Internal server error",
  });
});

// ===================================
// ì„œë²„ ì‹œìž‘
// ===================================

app.listen(PORT, () => {
  console.log("\nðŸš€ Week 5 AI Proxy Server Started!");
  console.log(`ðŸ“¡ Server: http://localhost:${PORT}`);
  console.log(`ðŸ¤– LLM Provider: ${process.env.LLM_PROVIDER || "gemini"}`);
  console.log(`â±ï¸  Rate Limit: 10 requests per 1 minute (í•™ìŠµìš© ì™„í™” ì„¤ì •)`);
  console.log("\nðŸ“‹ Available Endpoints:");
  console.log(`  GET  /                  - Health check`);
  console.log(`  POST /api/ai/generate   - General AI generation`);
  console.log(`  POST /api/ai/breakdown  - Task breakdown (preset prompt)`);
  console.log("\nðŸ’¡ Frontend URL: Open index.html in browser");
  console.log("âš ï¸  Remember: NEVER expose API keys in frontend!\n");
});

// Graceful shutdown
process.on("SIGTERM", () => {
  console.log("\nðŸ‘‹ Shutting down gracefully...");
  process.exit(0);
});

process.on("SIGINT", () => {
  console.log("\nðŸ‘‹ Shutting down gracefully...");
  process.exit(0);
});
